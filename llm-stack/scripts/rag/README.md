# LLM Stack - RAG System (Step 4)

This directory contains the **RAG (Retrieval-Augmented Generation) system** for the LLM Stack platform. This is **Step 4** of the project, which builds upon the normalized data exports from Step 3 to create a searchable vector index with AI-powered synthesis capabilities.

## ğŸ¯ **What is RAG?**

**RAG (Retrieval-Augmented Generation)** combines:
- **Vector Search**: Semantic similarity search through your documents and photos
- **AI Synthesis**: Llama-powered generation of intelligent answers from search results
- **Unified Interface**: Single system to search across all your data sources

## ğŸ—ï¸ **System Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   JSONL Data   â”‚â”€â”€â”€â–¶â”‚  Vector Index   â”‚â”€â”€â”€â–¶â”‚  Search Engine  â”‚
â”‚  (Step 3)      â”‚    â”‚   (ChromaDB)    â”‚    â”‚  + Web UI      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Embeddings     â”‚    â”‚   Llama AI      â”‚
                       â”‚ (Sentence-      â”‚    â”‚  Synthesis     â”‚
                       â”‚  Transformers)  â”‚    â”‚                â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ **Components Overview**

### **Core RAG Components**
- **`build-rag-index.py`** - Builds vector embeddings and ChromaDB index
- **`rag-search.py`** - Command-line search engine with Llama integration
- **`rag-web-interface.py`** - Beautiful web interface for searching

### **Supporting Files**
- **`run-rag.ps1`** - PowerShell wrapper for easy execution
- **`requirements.txt`** - Python dependencies
- **`README.md`** - This documentation

## ğŸš€ **Quick Start**

### **1. Install Dependencies**
```powershell
# Navigate to RAG directory
cd llm-stack/scripts/rag

# Install Python dependencies
.\run-rag.ps1 -InstallDeps
```

### **2. Build RAG Index**
```powershell
# Build index from Step 3 export data
.\run-rag.ps1 -Action build-index -InputFile ../export/exports/sample-combined-20250830-234124.jsonl
```

### **3. Test Search**
```powershell
# Test search functionality
.\run-rag.ps1 -Action search -Query "sample document"
```

### **4. Start Web Interface**
```powershell
# Start web interface
.\run-rag.ps1 -Action web -Port 5000
```

## ğŸ”§ **Detailed Usage**

### **Building the RAG Index**

The index builder processes your JSONL data and creates:
- **Vector embeddings** using sentence transformers
- **ChromaDB index** for fast similarity search
- **Metadata indexing** for filtering and organization

```bash
# Direct Python execution
python build-rag-index.py --input ../export/exports/sample-combined-20250830-234124.jsonl --output-dir rag-index

# With custom embedding model
python build-rag-index.py --input data.jsonl --model all-mpnet-base-v2 --output-dir custom-index
```

**What happens during indexing:**
1. **Load JSONL data** from Step 3 exports
2. **Create searchable content** by combining title, content, and metadata
3. **Generate embeddings** using the specified model
4. **Store in ChromaDB** with metadata for filtering
5. **Save statistics** and index information

### **Searching the Index**

The search engine provides multiple search modes:

```bash
# Basic search
python rag-search.py --index-dir rag-index --query "invoice documents"

# Filter by document type
python rag-search.py --index-dir rag-index --query "nature photos" --doc-type photo

# Filter by source
python rag-search.py --index-dir rag-index --query "correspondence" --source paperless

# With AI synthesis (requires Llama model)
python rag-search.py --index-dir rag-index --query "what documents do I have?" --llama-model path/to/model.gguf --synthesize
```

**Search Features:**
- **Semantic similarity** - Finds related content even with different wording
- **Metadata filtering** - Filter by source, type, tags, etc.
- **Relevance scoring** - Results ranked by similarity
- **Similar document finding** - Find documents similar to a specific item

### **Web Interface**

The web interface provides a beautiful, responsive search experience:

```bash
# Start web interface
python rag-web-interface.py --index-dir rag-index --port 5000

# With Llama integration
python rag-web-interface.py --index-dir rag-index --port 5000 --llama-model path/to/model.gguf
```

**Web Interface Features:**
- **Modern, responsive design** with gradient backgrounds
- **Real-time search** with instant results
- **Advanced filtering** by document type and source
- **Result previews** with metadata display
- **API endpoints** for programmatic access
- **Mobile-friendly** responsive layout

## ğŸ¤– **Llama AI Integration**

### **AI Synthesis Capabilities**

When you provide a Llama model, the system can:
- **Generate intelligent answers** based on search results
- **Synthesize information** from multiple documents
- **Provide context-aware responses** to complex queries
- **Summarize findings** in natural language

### **Getting Llama Models**

1. **Download GGUF models** from Hugging Face:
   ```bash
   # Example: Llama 2 7B Chat
   wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
   ```

2. **Use with RAG system**:
   ```bash
   .\run-rag.ps1 -Action search -Query "summarize my documents" -LlamaModel llama-2-7b-chat.Q4_K_M.gguf
   ```

### **AI Synthesis Examples**

```bash
# Ask complex questions
Query: "What financial documents do I have from last year?"
AI Response: "Based on your document collection, you have several financial documents from last year including..."

# Request summaries
Query: "Summarize the key points from my correspondence"
AI Response: "From your correspondence documents, the main themes include..."

# Get insights
Query: "What patterns do you see in my photo collection?"
AI Response: "Your photo collection shows several patterns including..."
```

## ğŸ“Š **Data Processing**

### **Content Preparation**

The system intelligently prepares your data for search:

**Documents (Paperless):**
- Title + extracted text content
- Tags, correspondent, document type
- File metadata (size, pages, language)

**Photos (PhotoPrism):**
- AI-generated descriptions and titles
- Labels, faces, location data
- Camera and technical metadata
- Album and tag information

### **Embedding Generation**

- **Model**: `all-MiniLM-L6-v2` (default) - Fast, accurate, lightweight
- **Vector dimensions**: 384
- **Processing**: Batch processing with progress bars
- **Storage**: ChromaDB with metadata indexing

### **Search Quality**

- **Semantic understanding** - Finds related concepts, not just keywords
- **Metadata awareness** - Leverages tags, types, and other structured data
- **Relevance scoring** - Results ranked by similarity to query
- **Context preservation** - Maintains document relationships and metadata

## ğŸ” **Search Examples**

### **Document Searches**
```bash
# Find invoices
Query: "invoice payment"
Results: Invoice documents, payment receipts, financial statements

# Find correspondence
Query: "email from John"
Results: Emails, letters, messages from John

# Find by date
Query: "documents from March 2024"
Results: All documents created in March 2024
```

### **Photo Searches**
```bash
# Find nature photos
Query: "landscape nature"
Results: Outdoor photos, nature scenes, landscape photography

# Find people
Query: "family photos"
Results: Photos with faces, family gatherings, portraits

# Find by location
Query: "vacation photos"
Results: Photos from specific locations, travel images
```

### **Cross-Source Searches**
```bash
# Find everything about a topic
Query: "project Alpha"
Results: Documents mentioning "Alpha", photos labeled with "Alpha", related correspondence

# Find by time period
Query: "Q1 2024"
Results: All documents and photos from Q1 2024
```

## âš™ï¸ **Configuration & Customization**

### **Environment Variables**
```bash
# ChromaDB settings
export CHROMA_HOST=localhost
export CHROMA_PORT=8000

# Model settings
export SENTENCE_TRANSFORMER_MODEL=all-mpnet-base-v2
export LLAMA_MODEL_PATH=/path/to/model.gguf
```

### **Custom Embedding Models**

You can use different sentence transformer models:

```bash
# Better quality, slower
python build-rag-index.py --model all-mpnet-base-v2

# Faster, good quality
python build-rag-index.py --model all-MiniLM-L6-v2

# Multilingual support
python build-rag-index.py --model paraphrase-multilingual-MiniLM-L12-v2
```

### **ChromaDB Configuration**

The system uses ChromaDB with these settings:
- **Persistence**: Data stored on disk
- **Telemetry**: Disabled for privacy
- **Collections**: Organized by document type
- **Metadata**: Rich indexing for filtering

## ğŸ“ˆ **Performance & Scaling**

### **Index Performance**
- **Small datasets** (< 1K documents): Near-instant search
- **Medium datasets** (1K-10K documents): Fast search with good accuracy
- **Large datasets** (> 10K documents): Optimized with batching and indexing

### **Memory Usage**
- **Embedding model**: ~100-200MB RAM
- **ChromaDB**: ~50-100MB base + document storage
- **Llama model**: Varies by model size (1GB-20GB+)

### **Optimization Tips**
1. **Use appropriate embedding models** for your use case
2. **Batch process** large datasets
3. **Monitor memory usage** during indexing
4. **Use SSD storage** for better ChromaDB performance

## ğŸ› **Troubleshooting**

### **Common Issues**

#### **Dependencies Installation**
```bash
# If you get import errors
pip install -r requirements.txt

# For specific packages
pip install sentence-transformers chromadb
```

#### **Memory Issues**
```bash
# Reduce batch size for large datasets
python build-rag-index.py --input data.jsonl --batch-size 100

# Use smaller embedding model
python build-rag-index.py --model all-MiniLM-L6-v2
```

#### **ChromaDB Issues**
```bash
# Reset index if corrupted
rm -rf rag-index/chroma_db
python build-rag-index.py --input data.jsonl
```

#### **Llama Integration Issues**
```bash
# Check model path
ls -la path/to/model.gguf

# Verify model format
file path/to/model.gguf

# Test model loading
python -c "from llama_cpp import Llama; Llama('path/to/model.gguf')"
```

### **Debug Mode**
```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# Run with verbose output
python build-rag-index.py --input data.jsonl --verbose
```

## ğŸ”„ **Integration with Other Steps**

### **Step 3 Integration**
- **Input**: JSONL files from export scripts
- **Format**: Normalized document and photo data
- **Metadata**: Rich context for search and filtering

### **Future Steps**
- **Step 5**: Advanced analytics and insights
- **Step 6**: Automated workflows and triggers
- **Step 7**: Multi-user access and collaboration

## ğŸ“š **API Reference**

### **REST API Endpoints**

The web interface provides these API endpoints:

```bash
# Search API
GET /api/search?q=query&n_results=10&doc_type=document

# Index information
GET /api/index-info

# AI synthesis
GET /api/synthesize?q=query
```

### **Python API**

```python
from rag_search import RAGSearchEngine

# Initialize
engine = RAGSearchEngine("rag-index")

# Search
results = engine.search("query", n_results=10)

# Get similar documents
similar = engine.get_similar_documents("doc_id")

# AI synthesis
synthesis = engine.synthesize_answer("query", results)
```

## ğŸ† **Success Metrics**

### **What Success Looks Like**
- âœ… **Fast search** - Results in under 1 second
- âœ… **Accurate results** - Relevant documents found
- âœ… **Rich metadata** - Proper filtering and organization
- âœ… **AI synthesis** - Intelligent answers from Llama
- âœ… **Web interface** - Beautiful, responsive search experience

### **Performance Benchmarks**
- **Indexing speed**: 100-1000 documents/minute
- **Search latency**: < 100ms for typical queries
- **Memory efficiency**: < 500MB for 10K documents
- **Storage**: ~1-5MB per 1000 documents

## ğŸš€ **Next Steps**

After completing Step 4:

1. **Test the system** with various queries
2. **Optimize performance** for your data size
3. **Integrate Llama models** for AI synthesis
4. **Customize the interface** for your needs
5. **Scale up** with larger datasets
6. **Proceed to Step 5** - Advanced Analytics

## ğŸ¤ **Contributing**

To enhance the RAG system:

1. **Add new embedding models** to the builder
2. **Enhance search algorithms** for better results
3. **Improve web interface** with new features
4. **Add new data sources** beyond Paperless/PhotoPrism
5. **Optimize performance** for large-scale deployments

## ğŸ“„ **License**

This project follows the same license as the main LLM Stack platform.

---

**Step 4 Status**: ğŸš€ **RAG System Complete - Ready for Use!**  
**Next**: ğŸ¯ **Step 5 - Advanced Analytics & Insights**
